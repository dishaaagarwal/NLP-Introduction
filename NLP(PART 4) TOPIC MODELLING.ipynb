{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "Each document in the corpus can have several topics .Topic is nothing but collection of words.\n",
    "Topics are then used to label each document for easy access.\n",
    "Refrence--https://www.youtube.com/watch?v=xvqsFTUsOmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages to import\n",
    "from gensim import matutils, models\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>zee</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 7469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aaah  aah  abc  \\\n",
       "ali           0             0                 0          0     0    0    1   \n",
       "anthony       0             0                 0          0     0    0    0   \n",
       "bill          1             0                 0          0     0    0    0   \n",
       "bo            0             1                 1          1     0    0    0   \n",
       "dave          0             0                 0          0     1    0    0   \n",
       "hasan         0             0                 0          0     0    0    0   \n",
       "jim           0             0                 0          0     0    0    0   \n",
       "joe           0             0                 0          0     0    0    0   \n",
       "john          0             0                 0          0     0    0    0   \n",
       "louis         0             0                 0          0     0    3    0   \n",
       "mike          0             0                 0          0     0    0    0   \n",
       "ricky         0             0                 0          0     0    0    0   \n",
       "\n",
       "         abcs  ability  abject  ...  zee  zen  zeppelin  zero  zillion  \\\n",
       "ali         0        0       0  ...    0    0         0     0        0   \n",
       "anthony     0        0       0  ...    0    0         0     0        0   \n",
       "bill        1        0       0  ...    0    0         0     1        1   \n",
       "bo          0        1       0  ...    0    0         0     1        0   \n",
       "dave        0        0       0  ...    0    0         0     0        0   \n",
       "hasan       0        0       0  ...    2    1         0     1        0   \n",
       "jim         0        0       0  ...    0    0         0     0        0   \n",
       "joe         0        0       0  ...    0    0         0     0        0   \n",
       "john        0        0       0  ...    0    0         0     0        0   \n",
       "louis       0        0       0  ...    0    0         0     2        0   \n",
       "mike        0        0       0  ...    0    0         2     1        0   \n",
       "ricky       0        1       1  ...    0    0         0     0        0   \n",
       "\n",
       "         zombie  zombies  zoning  zoo  éclair  \n",
       "ali           1        0       0    0       0  \n",
       "anthony       0        0       0    0       0  \n",
       "bill          1        1       1    0       0  \n",
       "bo            0        0       0    0       0  \n",
       "dave          0        0       0    0       0  \n",
       "hasan         0        0       0    0       0  \n",
       "jim           0        0       0    0       0  \n",
       "joe           0        0       0    0       0  \n",
       "john          0        0       0    0       1  \n",
       "louis         0        0       0    0       0  \n",
       "mike          0        0       0    0       0  \n",
       "ricky         0        0       0    1       0  \n",
       "\n",
       "[12 rows x 7469 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the file.\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The i/p should be in the form of transpose of dtm-> Term Document Matrix\n",
    "\n",
    "tdm=data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT 1(ALL TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# We are going to change the format of the tdm for Gemsim. Df->Sparse Matrix->gensim corpus\n",
    "\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)#Sparse Matrix\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)# gensim format Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))# Count Vectorizer file we earlier created\n",
    "\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())# Dictionary for gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:04:07,192 : INFO : using symmetric alpha at 0.5\n",
      "2020-05-02 23:04:07,208 : INFO : using symmetric eta at 0.5\n",
      "2020-05-02 23:04:07,228 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:04:07,240 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:04:08,272 : INFO : -9.394 per-word bound, 672.8 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:08,276 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:04:08,428 : INFO : topic #0 (0.500): 0.006*\"fucking\" + 0.005*\"say\" + 0.005*\"shit\" + 0.005*\"theyre\" + 0.004*\"man\" + 0.004*\"fuck\" + 0.004*\"didnt\" + 0.004*\"going\" + 0.004*\"want\" + 0.004*\"cause\"\n",
      "2020-05-02 23:04:08,432 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"hes\" + 0.005*\"day\" + 0.005*\"want\" + 0.005*\"theyre\" + 0.005*\"didnt\"\n",
      "2020-05-02 23:04:08,436 : INFO : topic diff=1.087188, rho=1.000000\n",
      "2020-05-02 23:04:09,152 : INFO : -8.044 per-word bound, 264.0 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:09,152 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:04:09,264 : INFO : topic #0 (0.500): 0.005*\"say\" + 0.005*\"fucking\" + 0.004*\"love\" + 0.004*\"dad\" + 0.004*\"want\" + 0.004*\"going\" + 0.004*\"did\" + 0.004*\"shit\" + 0.004*\"shes\" + 0.004*\"man\"\n",
      "2020-05-02 23:04:09,264 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.005*\"say\" + 0.005*\"theyre\" + 0.005*\"going\" + 0.005*\"hes\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"thing\"\n",
      "2020-05-02 23:04:09,268 : INFO : topic diff=0.268601, rho=0.577350\n",
      "2020-05-02 23:04:09,779 : INFO : -7.968 per-word bound, 250.4 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:09,779 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:04:09,875 : INFO : topic #0 (0.500): 0.005*\"dad\" + 0.005*\"love\" + 0.005*\"say\" + 0.005*\"want\" + 0.004*\"going\" + 0.004*\"shes\" + 0.004*\"hey\" + 0.004*\"did\" + 0.004*\"fucking\" + 0.003*\"good\"\n",
      "2020-05-02 23:04:09,875 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"thing\" + 0.005*\"cause\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:09,879 : INFO : topic diff=0.176053, rho=0.500000\n",
      "2020-05-02 23:04:10,295 : INFO : -7.939 per-word bound, 245.4 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:10,299 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:04:10,391 : INFO : topic #0 (0.500): 0.005*\"dad\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.004*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.003*\"little\" + 0.003*\"good\"\n",
      "2020-05-02 23:04:10,391 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"thing\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:10,395 : INFO : topic diff=0.099862, rho=0.447214\n",
      "2020-05-02 23:04:10,783 : INFO : -7.930 per-word bound, 243.8 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:10,787 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:04:10,883 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.004*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"good\"\n",
      "2020-05-02 23:04:10,883 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:10,887 : INFO : topic diff=0.059298, rho=0.408248\n",
      "2020-05-02 23:04:11,267 : INFO : -7.926 per-word bound, 243.2 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:11,271 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:04:11,363 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.004*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"good\"\n",
      "2020-05-02 23:04:11,367 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:11,367 : INFO : topic diff=0.036305, rho=0.377964\n",
      "2020-05-02 23:04:11,751 : INFO : -7.925 per-word bound, 243.0 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:11,755 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:04:11,883 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"good\"\n",
      "2020-05-02 23:04:11,887 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:11,891 : INFO : topic diff=0.022802, rho=0.353553\n",
      "2020-05-02 23:04:12,495 : INFO : -7.924 per-word bound, 242.9 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:12,499 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:04:12,651 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"mom\"\n",
      "2020-05-02 23:04:12,655 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:12,659 : INFO : topic diff=0.014654, rho=0.333333\n",
      "2020-05-02 23:04:13,363 : INFO : -7.924 per-word bound, 242.8 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:13,367 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:04:13,515 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"mom\"\n",
      "2020-05-02 23:04:13,523 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:13,527 : INFO : topic diff=0.009619, rho=0.316228\n",
      "2020-05-02 23:04:14,274 : INFO : -7.924 per-word bound, 242.8 perplexity estimate based on a held-out corpus of 12 documents with 41270 words\n",
      "2020-05-02 23:04:14,278 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:04:14,446 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"mom\"\n",
      "2020-05-02 23:04:14,450 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n",
      "2020-05-02 23:04:14,454 : INFO : topic diff=0.006436, rho=0.301511\n",
      "2020-05-02 23:04:14,478 : INFO : topic #0 (0.500): 0.006*\"dad\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"mom\"\n",
      "2020-05-02 23:04:14,482 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"dad\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\" + 0.003*\"mom\"'),\n",
       " (1,\n",
       "  '0.009*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Latent Dirichlet Allocation(LDA)\n",
    "lda=models.LdaModel(corpus=corpus,num_topics=2,id2word=id2word,passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT 2(ONLY NOUNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>bo what old macdonald had a farm e i e i o and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats up davis whats up im home i had to bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "bo       bo what old macdonald had a farm e i e i o and...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "hasan      whats up davis whats up im home i had to bri...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagger is to assign linguistic (mostly grammatical) information to tokens\n",
    "#List for various grammer short code--https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_nouns(text):\n",
    "    is_noun=lambda x: x[:2]=='NN' # Noun \n",
    "    tokenized=word_tokenize(text)\n",
    "    all_nouns=[ word for word,word_info in pos_tag(tokenized) if is_noun(word_info)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>bo what old macdonald had a farm e i e i o and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats up davis whats up im home i had to bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "bo       bo what old macdonald had a farm e i e i o and...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "hasan      whats up davis whats up im home i had to bri...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>macdonald farm e i o farm pig e i i snort macd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats davis whats home i netflix la york i son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fuck thanks phone fuckfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>petunia thats hello hello chicago thank crowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "bo       macdonald farm e i o farm pig e i i snort macd...\n",
       "dave     jokes living stare work profound train thought...\n",
       "hasan    whats davis whats home i netflix la york i son...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "joe      ladies gentlemen joe fuck thanks phone fuckfac...\n",
       "john     petunia thats hello hello chicago thank crowd ...\n",
       "louis    music lets lights lights thank i i place place...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "ricky    hello thank fuck thank im gon youre weve money..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the Function\n",
    "data_nouns=pd.DataFrame(data_clean.transcript.apply(get_nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 4627 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  ability  \\\n",
       "ali                 0                 0          0    0    1     0        0   \n",
       "anthony             0                 0          0    0    0     0        0   \n",
       "bill                0                 0          0    0    0     1        0   \n",
       "bo                  1                 1          1    0    0     0        1   \n",
       "dave                0                 0          0    0    0     0        0   \n",
       "hasan               0                 0          0    0    0     0        0   \n",
       "jim                 0                 0          0    0    0     0        0   \n",
       "joe                 0                 0          0    0    0     0        0   \n",
       "john                0                 0          0    0    0     0        0   \n",
       "louis               0                 0          0    3    0     0        0   \n",
       "mike                0                 0          0    0    0     0        0   \n",
       "ricky               0                 0          0    0    0     0        1   \n",
       "\n",
       "         abortion  abortions  abuse  ...  yummy  ze  zealand  zee  zeppelin  \\\n",
       "ali             0          0      0  ...      0   0        0    0         0   \n",
       "anthony         2          0      0  ...      0   0       10    0         0   \n",
       "bill            0          0      0  ...      0   1        0    0         0   \n",
       "bo              0          0      0  ...      0   0        0    0         0   \n",
       "dave            0          1      0  ...      0   0        0    0         0   \n",
       "hasan           0          0      0  ...      0   0        0    1         0   \n",
       "jim             0          0      0  ...      0   0        0    0         0   \n",
       "joe             0          0      1  ...      0   0        0    0         0   \n",
       "john            0          0      0  ...      0   0        0    0         0   \n",
       "louis           0          0      0  ...      0   0        0    0         0   \n",
       "mike            0          0      0  ...      0   0        0    0         2   \n",
       "ricky           0          0      0  ...      1   0        0    0         0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  éclair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 4627 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said','em']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items()) # Rememeber to always use CounVectorizer Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:04:53,687 : INFO : using symmetric alpha at 0.5\n",
      "2020-05-02 23:04:53,695 : INFO : using symmetric eta at 0.5\n",
      "2020-05-02 23:04:53,707 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:04:53,719 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:04:54,451 : INFO : -8.941 per-word bound, 491.6 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:54,455 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:04:54,687 : INFO : topic #0 (0.500): 0.009*\"thing\" + 0.007*\"shit\" + 0.007*\"life\" + 0.007*\"man\" + 0.007*\"way\" + 0.007*\"hes\" + 0.007*\"fuck\" + 0.006*\"lot\" + 0.006*\"guy\" + 0.006*\"day\"\n",
      "2020-05-02 23:04:54,691 : INFO : topic #1 (0.500): 0.011*\"day\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"life\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"cause\" + 0.006*\"gon\" + 0.006*\"way\" + 0.005*\"fuck\"\n",
      "2020-05-02 23:04:54,695 : INFO : topic diff=0.950012, rho=1.000000\n",
      "2020-05-02 23:04:55,439 : INFO : -7.755 per-word bound, 216.1 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:55,443 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:04:55,607 : INFO : topic #0 (0.500): 0.009*\"thing\" + 0.008*\"life\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.006*\"fuck\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"day\" + 0.005*\"guy\"\n",
      "2020-05-02 23:04:55,611 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.008*\"thing\" + 0.008*\"shit\" + 0.007*\"cause\" + 0.007*\"man\" + 0.007*\"gon\" + 0.007*\"hes\" + 0.007*\"life\" + 0.006*\"way\" + 0.006*\"guy\"\n",
      "2020-05-02 23:04:55,615 : INFO : topic diff=0.293007, rho=0.577350\n",
      "2020-05-02 23:04:56,074 : INFO : -7.665 per-word bound, 202.9 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:56,074 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:04:56,186 : INFO : topic #0 (0.500): 0.008*\"life\" + 0.008*\"thing\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.006*\"lot\" + 0.006*\"day\" + 0.006*\"fuck\" + 0.006*\"way\" + 0.006*\"dad\"\n",
      "2020-05-02 23:04:56,190 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.008*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"gon\" + 0.007*\"hes\" + 0.007*\"way\" + 0.006*\"life\" + 0.006*\"guy\"\n",
      "2020-05-02 23:04:56,194 : INFO : topic diff=0.199616, rho=0.500000\n",
      "2020-05-02 23:04:56,550 : INFO : -7.624 per-word bound, 197.3 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:56,554 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:04:56,646 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.008*\"thing\" + 0.007*\"man\" + 0.007*\"hes\" + 0.006*\"day\" + 0.006*\"way\" + 0.006*\"dad\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:56,650 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:56,654 : INFO : topic diff=0.125643, rho=0.447214\n",
      "2020-05-02 23:04:56,966 : INFO : -7.606 per-word bound, 194.8 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:56,966 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:04:57,078 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.006*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:57,082 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:57,086 : INFO : topic diff=0.075596, rho=0.408248\n",
      "2020-05-02 23:04:57,442 : INFO : -7.599 per-word bound, 193.9 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:57,442 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:04:57,534 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.006*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:57,538 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:57,542 : INFO : topic diff=0.046645, rho=0.377964\n",
      "2020-05-02 23:04:57,838 : INFO : -7.596 per-word bound, 193.5 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:57,842 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:04:58,022 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.006*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:58,034 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:58,038 : INFO : topic diff=0.029555, rho=0.353553\n",
      "2020-05-02 23:04:58,634 : INFO : -7.595 per-word bound, 193.4 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:58,638 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:04:58,718 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:58,718 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:58,722 : INFO : topic diff=0.019146, rho=0.333333\n",
      "2020-05-02 23:04:58,982 : INFO : -7.595 per-word bound, 193.3 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:58,982 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:04:59,062 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:59,062 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:59,066 : INFO : topic diff=0.012651, rho=0.316228\n",
      "2020-05-02 23:04:59,350 : INFO : -7.594 per-word bound, 193.3 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:59,354 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:04:59,446 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:59,450 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:59,454 : INFO : topic diff=0.008512, rho=0.301511\n",
      "2020-05-02 23:04:59,462 : INFO : topic #0 (0.500): 0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"\n",
      "2020-05-02 23:04:59,466 : INFO : topic #1 (0.500): 0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"life\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"dad\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"fuck\"'),\n",
       " (1,\n",
       "  '0.010*\"day\" + 0.009*\"thing\" + 0.008*\"cause\" + 0.007*\"shit\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"fuck\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:04:59,494 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-05-02 23:04:59,498 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-05-02 23:04:59,502 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:04:59,510 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:04:59,773 : INFO : -9.162 per-word bound, 572.9 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:04:59,773 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:04:59,857 : INFO : topic #0 (0.333): 0.010*\"day\" + 0.007*\"man\" + 0.006*\"way\" + 0.006*\"cause\" + 0.006*\"shit\" + 0.006*\"thing\" + 0.006*\"shes\" + 0.005*\"life\" + 0.005*\"theyre\" + 0.005*\"home\"\n",
      "2020-05-02 23:04:59,857 : INFO : topic #1 (0.333): 0.009*\"man\" + 0.009*\"shit\" + 0.008*\"fuck\" + 0.008*\"day\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"life\" + 0.006*\"guy\" + 0.006*\"lot\"\n",
      "2020-05-02 23:04:59,861 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.008*\"life\" + 0.008*\"day\" + 0.008*\"hes\" + 0.007*\"shit\" + 0.007*\"cause\" + 0.006*\"gon\" + 0.006*\"man\" + 0.006*\"fuck\" + 0.006*\"guy\"\n",
      "2020-05-02 23:04:59,861 : INFO : topic diff=1.146935, rho=1.000000\n",
      "2020-05-02 23:05:00,177 : INFO : -7.907 per-word bound, 240.0 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:00,181 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:05:00,261 : INFO : topic #0 (0.333): 0.008*\"day\" + 0.006*\"way\" + 0.006*\"man\" + 0.005*\"cause\" + 0.005*\"stuff\" + 0.005*\"life\" + 0.004*\"thing\" + 0.004*\"shes\" + 0.004*\"shit\" + 0.004*\"hes\"\n",
      "2020-05-02 23:05:00,265 : INFO : topic #1 (0.333): 0.011*\"shit\" + 0.010*\"man\" + 0.009*\"fuck\" + 0.007*\"way\" + 0.007*\"lot\" + 0.007*\"hes\" + 0.006*\"life\" + 0.006*\"day\" + 0.006*\"thing\" + 0.006*\"guy\"\n",
      "2020-05-02 23:05:00,269 : INFO : topic #2 (0.333): 0.012*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"cause\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"gon\" + 0.006*\"way\" + 0.006*\"man\"\n",
      "2020-05-02 23:05:00,269 : INFO : topic diff=0.390671, rho=0.577350\n",
      "2020-05-02 23:05:00,425 : INFO : -7.747 per-word bound, 214.8 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:00,425 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:05:00,465 : INFO : topic #0 (0.333): 0.007*\"day\" + 0.006*\"stuff\" + 0.006*\"way\" + 0.005*\"man\" + 0.005*\"bo\" + 0.005*\"cause\" + 0.005*\"point\" + 0.005*\"repeat\" + 0.004*\"eye\" + 0.004*\"hes\"\n",
      "2020-05-02 23:05:00,469 : INFO : topic #1 (0.333): 0.012*\"shit\" + 0.011*\"man\" + 0.010*\"fuck\" + 0.008*\"lot\" + 0.007*\"way\" + 0.006*\"hes\" + 0.006*\"life\" + 0.006*\"cause\" + 0.006*\"day\" + 0.005*\"women\"\n",
      "2020-05-02 23:05:00,469 : INFO : topic #2 (0.333): 0.012*\"thing\" + 0.010*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"cause\" + 0.007*\"shit\" + 0.007*\"guy\" + 0.006*\"way\" + 0.006*\"man\" + 0.006*\"gon\"\n",
      "2020-05-02 23:05:00,473 : INFO : topic diff=0.314525, rho=0.500000\n",
      "2020-05-02 23:05:00,617 : INFO : -7.654 per-word bound, 201.4 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:00,617 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:05:00,657 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.006*\"way\" + 0.006*\"point\" + 0.006*\"bo\" + 0.005*\"day\" + 0.005*\"cause\" + 0.005*\"repeat\" + 0.005*\"man\" + 0.004*\"eye\" + 0.004*\"night\"\n",
      "2020-05-02 23:05:00,661 : INFO : topic #1 (0.333): 0.013*\"shit\" + 0.011*\"man\" + 0.010*\"fuck\" + 0.008*\"lot\" + 0.006*\"way\" + 0.006*\"cause\" + 0.006*\"hes\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"life\"\n",
      "2020-05-02 23:05:00,661 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"shit\" + 0.007*\"cause\" + 0.007*\"way\" + 0.007*\"guy\" + 0.006*\"man\" + 0.006*\"dad\"\n",
      "2020-05-02 23:05:00,665 : INFO : topic diff=0.231216, rho=0.447214\n",
      "2020-05-02 23:05:00,805 : INFO : -7.608 per-word bound, 195.1 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:00,805 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:05:00,849 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.006*\"way\" + 0.006*\"point\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.005*\"repeat\" + 0.005*\"day\" + 0.005*\"eye\" + 0.005*\"man\" + 0.005*\"kind\"\n",
      "2020-05-02 23:05:00,853 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"man\" + 0.011*\"fuck\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"way\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"hes\"\n",
      "2020-05-02 23:05:00,853 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"shit\" + 0.007*\"way\" + 0.006*\"guy\" + 0.006*\"cause\" + 0.006*\"man\" + 0.006*\"dad\"\n",
      "2020-05-02 23:05:00,857 : INFO : topic diff=0.144123, rho=0.408248\n",
      "2020-05-02 23:05:00,985 : INFO : -7.591 per-word bound, 192.9 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:00,985 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:05:01,025 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.006*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.005*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"day\" + 0.005*\"night\"\n",
      "2020-05-02 23:05:01,025 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"man\" + 0.011*\"fuck\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"way\" + 0.006*\"hes\"\n",
      "2020-05-02 23:05:01,029 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"cause\" + 0.006*\"man\" + 0.006*\"dad\"\n",
      "2020-05-02 23:05:01,029 : INFO : topic diff=0.092284, rho=0.377964\n",
      "2020-05-02 23:05:01,165 : INFO : -7.584 per-word bound, 191.9 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:01,165 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:05:01,201 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.006*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.006*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"night\" + 0.005*\"id\"\n",
      "2020-05-02 23:05:01,205 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"fuck\" + 0.011*\"man\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"way\" + 0.006*\"hes\"\n",
      "2020-05-02 23:05:01,205 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"dad\" + 0.006*\"man\" + 0.006*\"cause\"\n",
      "2020-05-02 23:05:01,209 : INFO : topic diff=0.060163, rho=0.353553\n",
      "2020-05-02 23:05:01,349 : INFO : -7.581 per-word bound, 191.5 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:01,349 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:05:01,381 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.007*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.006*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"night\" + 0.005*\"id\"\n",
      "2020-05-02 23:05:01,381 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"fuck\" + 0.011*\"man\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"way\" + 0.006*\"hes\"\n",
      "2020-05-02 23:05:01,389 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"dad\" + 0.006*\"man\" + 0.006*\"cause\"\n",
      "2020-05-02 23:05:01,389 : INFO : topic diff=0.039824, rho=0.333333\n",
      "2020-05-02 23:05:01,525 : INFO : -7.580 per-word bound, 191.3 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:01,529 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:05:01,565 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.007*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.006*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"night\" + 0.005*\"id\"\n",
      "2020-05-02 23:05:01,569 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"fuck\" + 0.011*\"man\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"way\" + 0.006*\"dude\"\n",
      "2020-05-02 23:05:01,573 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"dad\" + 0.006*\"man\" + 0.006*\"cause\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:01,577 : INFO : topic diff=0.026745, rho=0.316228\n",
      "2020-05-02 23:05:01,717 : INFO : -7.579 per-word bound, 191.2 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:01,717 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:05:01,757 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.007*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.006*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"night\" + 0.005*\"id\"\n",
      "2020-05-02 23:05:01,761 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"fuck\" + 0.011*\"man\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"dude\" + 0.006*\"way\"\n",
      "2020-05-02 23:05:01,761 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"dad\" + 0.006*\"man\" + 0.006*\"cause\"\n",
      "2020-05-02 23:05:01,765 : INFO : topic diff=0.018219, rho=0.301511\n",
      "2020-05-02 23:05:01,769 : INFO : topic #0 (0.333): 0.007*\"stuff\" + 0.007*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.006*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"night\" + 0.005*\"id\"\n",
      "2020-05-02 23:05:01,769 : INFO : topic #1 (0.333): 0.014*\"shit\" + 0.011*\"fuck\" + 0.011*\"man\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"dude\" + 0.006*\"way\"\n",
      "2020-05-02 23:05:01,773 : INFO : topic #2 (0.333): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"dad\" + 0.006*\"man\" + 0.006*\"cause\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"stuff\" + 0.007*\"point\" + 0.006*\"way\" + 0.006*\"bo\" + 0.006*\"cause\" + 0.006*\"repeat\" + 0.005*\"kind\" + 0.005*\"eye\" + 0.005*\"night\" + 0.005*\"id\"'),\n",
       " (1,\n",
       "  '0.014*\"shit\" + 0.011*\"fuck\" + 0.011*\"man\" + 0.009*\"lot\" + 0.006*\"cause\" + 0.006*\"women\" + 0.006*\"gon\" + 0.006*\"theyre\" + 0.006*\"dude\" + 0.006*\"way\"'),\n",
       " (2,\n",
       "  '0.011*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"way\" + 0.007*\"shit\" + 0.006*\"guy\" + 0.006*\"dad\" + 0.006*\"man\" + 0.006*\"cause\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 3 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:01,785 : INFO : using symmetric alpha at 0.25\n",
      "2020-05-02 23:05:01,797 : INFO : using symmetric eta at 0.25\n",
      "2020-05-02 23:05:01,805 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:05:01,813 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:05:02,013 : INFO : -9.420 per-word bound, 684.8 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:02,013 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:05:02,077 : INFO : topic #0 (0.250): 0.008*\"thing\" + 0.007*\"fuck\" + 0.007*\"way\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"shit\" + 0.007*\"things\" + 0.006*\"day\" + 0.006*\"gon\" + 0.006*\"cause\"\n",
      "2020-05-02 23:05:02,077 : INFO : topic #1 (0.250): 0.008*\"man\" + 0.008*\"day\" + 0.007*\"thing\" + 0.006*\"shit\" + 0.006*\"women\" + 0.006*\"cause\" + 0.006*\"guy\" + 0.006*\"theyre\" + 0.005*\"life\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:02,081 : INFO : topic #2 (0.250): 0.009*\"thing\" + 0.008*\"life\" + 0.008*\"cause\" + 0.008*\"hes\" + 0.008*\"day\" + 0.008*\"shit\" + 0.007*\"man\" + 0.006*\"way\" + 0.006*\"kid\" + 0.006*\"shes\"\n",
      "2020-05-02 23:05:02,081 : INFO : topic #3 (0.250): 0.011*\"day\" + 0.009*\"shit\" + 0.009*\"life\" + 0.008*\"thing\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"hes\" + 0.006*\"guy\" + 0.006*\"lot\"\n",
      "2020-05-02 23:05:02,085 : INFO : topic diff=1.378718, rho=1.000000\n",
      "2020-05-02 23:05:02,229 : INFO : -8.029 per-word bound, 261.2 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:02,229 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:05:02,281 : INFO : topic #0 (0.250): 0.008*\"thing\" + 0.007*\"hes\" + 0.007*\"man\" + 0.006*\"day\" + 0.006*\"things\" + 0.006*\"way\" + 0.006*\"joke\" + 0.005*\"years\" + 0.005*\"stuff\" + 0.005*\"fuck\"\n",
      "2020-05-02 23:05:02,281 : INFO : topic #1 (0.250): 0.007*\"cause\" + 0.006*\"day\" + 0.006*\"kind\" + 0.006*\"thing\" + 0.006*\"point\" + 0.006*\"guy\" + 0.005*\"night\" + 0.005*\"gon\" + 0.005*\"way\" + 0.005*\"man\"\n",
      "2020-05-02 23:05:02,285 : INFO : topic #2 (0.250): 0.009*\"life\" + 0.009*\"shit\" + 0.009*\"thing\" + 0.008*\"day\" + 0.007*\"hes\" + 0.007*\"lot\" + 0.007*\"man\" + 0.007*\"cause\" + 0.007*\"way\" + 0.007*\"dad\"\n",
      "2020-05-02 23:05:02,285 : INFO : topic #3 (0.250): 0.011*\"shit\" + 0.010*\"day\" + 0.010*\"fuck\" + 0.009*\"thing\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.007*\"cause\"\n",
      "2020-05-02 23:05:02,289 : INFO : topic diff=0.721553, rho=0.577350\n",
      "2020-05-02 23:05:02,433 : INFO : -7.694 per-word bound, 207.1 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:02,433 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:05:02,473 : INFO : topic #0 (0.250): 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"joke\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"stuff\" + 0.006*\"things\" + 0.006*\"years\" + 0.006*\"way\" + 0.005*\"bo\"\n",
      "2020-05-02 23:05:02,477 : INFO : topic #1 (0.250): 0.008*\"cause\" + 0.007*\"point\" + 0.007*\"kind\" + 0.006*\"way\" + 0.005*\"night\" + 0.005*\"guy\" + 0.005*\"id\" + 0.005*\"day\" + 0.005*\"thing\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:02,477 : INFO : topic #2 (0.250): 0.009*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"thing\" + 0.008*\"lot\" + 0.007*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"cause\"\n",
      "2020-05-02 23:05:02,481 : INFO : topic #3 (0.250): 0.011*\"shit\" + 0.011*\"fuck\" + 0.010*\"day\" + 0.010*\"thing\" + 0.009*\"man\" + 0.009*\"guy\" + 0.008*\"life\" + 0.008*\"gon\" + 0.007*\"cause\" + 0.007*\"hes\"\n",
      "2020-05-02 23:05:02,485 : INFO : topic diff=0.416480, rho=0.500000\n",
      "2020-05-02 23:05:02,609 : INFO : -7.607 per-word bound, 195.0 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:02,613 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:05:02,645 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"thing\" + 0.007*\"stuff\" + 0.007*\"day\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"bo\" + 0.006*\"things\" + 0.006*\"id\"\n",
      "2020-05-02 23:05:02,649 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.008*\"point\" + 0.007*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"night\" + 0.005*\"car\" + 0.005*\"guy\" + 0.005*\"jenny\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:02,649 : INFO : topic #2 (0.250): 0.009*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"thing\" + 0.008*\"lot\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"cause\"\n",
      "2020-05-02 23:05:02,653 : INFO : topic #3 (0.250): 0.011*\"fuck\" + 0.011*\"shit\" + 0.010*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"gon\" + 0.008*\"life\" + 0.008*\"cause\" + 0.007*\"hes\"\n",
      "2020-05-02 23:05:02,653 : INFO : topic diff=0.250128, rho=0.447214\n",
      "2020-05-02 23:05:02,805 : INFO : -7.577 per-word bound, 191.0 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:02,805 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:05:02,841 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.007*\"hes\" + 0.007*\"thing\" + 0.007*\"stuff\" + 0.007*\"day\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"bo\" + 0.006*\"things\" + 0.006*\"id\"\n",
      "2020-05-02 23:05:02,845 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.007*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"night\" + 0.006*\"jenny\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:02,849 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"thing\" + 0.008*\"lot\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:02,849 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"hes\"\n",
      "2020-05-02 23:05:02,849 : INFO : topic diff=0.154481, rho=0.408248\n",
      "2020-05-02 23:05:02,985 : INFO : -7.566 per-word bound, 189.4 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:02,985 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:05:03,017 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.008*\"hes\" + 0.007*\"thing\" + 0.007*\"stuff\" + 0.007*\"day\" + 0.006*\"man\" + 0.006*\"bo\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"things\"\n",
      "2020-05-02 23:05:03,025 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:03,029 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:03,029 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"\n",
      "2020-05-02 23:05:03,033 : INFO : topic diff=0.097313, rho=0.377964\n",
      "2020-05-02 23:05:03,177 : INFO : -7.561 per-word bound, 188.8 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:03,181 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:05:03,213 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.007*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"repeat\"\n",
      "2020-05-02 23:05:03,217 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:03,217 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:03,221 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"\n",
      "2020-05-02 23:05:03,225 : INFO : topic diff=0.062472, rho=0.353553\n",
      "2020-05-02 23:05:03,357 : INFO : -7.558 per-word bound, 188.5 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:03,357 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:05:03,393 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.007*\"day\" + 0.007*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"repeat\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:03,397 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:03,397 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:03,401 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"\n",
      "2020-05-02 23:05:03,401 : INFO : topic diff=0.040846, rho=0.333333\n",
      "2020-05-02 23:05:03,545 : INFO : -7.557 per-word bound, 188.3 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:03,545 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:05:03,581 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.007*\"day\" + 0.007*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"repeat\"\n",
      "2020-05-02 23:05:03,581 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:03,585 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:03,589 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"\n",
      "2020-05-02 23:05:03,589 : INFO : topic diff=0.027179, rho=0.316228\n",
      "2020-05-02 23:05:03,708 : INFO : -7.557 per-word bound, 188.3 perplexity estimate based on a held-out corpus of 12 documents with 19852 words\n",
      "2020-05-02 23:05:03,712 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:05:03,740 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.008*\"hes\" + 0.008*\"stuff\" + 0.007*\"thing\" + 0.007*\"day\" + 0.007*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"repeat\"\n",
      "2020-05-02 23:05:03,744 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:03,744 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:03,748 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"\n",
      "2020-05-02 23:05:03,748 : INFO : topic diff=0.018389, rho=0.301511\n",
      "2020-05-02 23:05:03,756 : INFO : topic #0 (0.250): 0.008*\"joke\" + 0.008*\"hes\" + 0.008*\"stuff\" + 0.007*\"thing\" + 0.007*\"day\" + 0.007*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"repeat\"\n",
      "2020-05-02 23:05:03,756 : INFO : topic #1 (0.250): 0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"\n",
      "2020-05-02 23:05:03,760 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"\n",
      "2020-05-02 23:05:03,760 : INFO : topic #3 (0.250): 0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"joke\" + 0.008*\"hes\" + 0.008*\"stuff\" + 0.007*\"thing\" + 0.007*\"day\" + 0.007*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"id\" + 0.006*\"repeat\"'),\n",
       " (1,\n",
       "  '0.009*\"cause\" + 0.009*\"point\" + 0.008*\"kind\" + 0.006*\"way\" + 0.006*\"id\" + 0.006*\"jenny\" + 0.006*\"night\" + 0.006*\"car\" + 0.005*\"guy\" + 0.005*\"gon\"'),\n",
       " (2,\n",
       "  '0.010*\"shit\" + 0.009*\"life\" + 0.009*\"day\" + 0.008*\"lot\" + 0.008*\"thing\" + 0.008*\"dad\" + 0.007*\"hes\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"mom\"'),\n",
       " (3,\n",
       "  '0.012*\"fuck\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.010*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"gon\" + 0.008*\"life\" + 0.007*\"theyre\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT 3(NOUNS & ADJECTIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns_adj(text):\n",
    "    is_noun=lambda x: x[:2]=='NN' or x[:2]=='JJ' # Noun \n",
    "    tokenized=word_tokenize(text)\n",
    "    all_nouns=[ word for word,word_info in pos_tag(tokenized) if is_noun(word_info)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>old macdonald farm e i i o farm pig e i i snor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats davis whats im home i netflix special la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fuck san francisco thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>right petunia august thats good right hello he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "bo       old macdonald farm e i i o farm pig e i i snor...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "hasan    whats davis whats im home i netflix special la...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "joe      ladies gentlemen joe fuck san francisco thanks...\n",
       "john     right petunia august thats good right hello he...\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "ricky    hello great thank fuck thank lovely welcome im..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(get_nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 5583 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  \\\n",
       "ali           0             0                 0          0    0    1     0   \n",
       "anthony       0             0                 0          0    0    0     0   \n",
       "bill          1             0                 0          0    0    0     1   \n",
       "bo            0             1                 1          1    0    0     0   \n",
       "dave          0             0                 0          0    0    0     0   \n",
       "hasan         0             0                 0          0    0    0     0   \n",
       "jim           0             0                 0          0    0    0     0   \n",
       "joe           0             0                 0          0    0    0     0   \n",
       "john          0             0                 0          0    0    0     0   \n",
       "louis         0             0                 0          0    3    0     0   \n",
       "mike          0             0                 0          0    0    0     0   \n",
       "ricky         0             0                 0          0    0    0     0   \n",
       "\n",
       "         ability  abject  able  ...  ze  zealand  zee  zeppelin  zero  \\\n",
       "ali            0       0     2  ...   0        0    0         0     0   \n",
       "anthony        0       0     0  ...   0       10    0         0     0   \n",
       "bill           0       0     1  ...   1        0    0         0     0   \n",
       "bo             1       0     0  ...   0        0    0         0     1   \n",
       "dave           0       0     0  ...   0        0    0         0     0   \n",
       "hasan          0       0     1  ...   0        0    2         0     0   \n",
       "jim            0       0     1  ...   0        0    0         0     0   \n",
       "joe            0       0     2  ...   0        0    0         0     0   \n",
       "john           0       0     3  ...   0        0    0         0     0   \n",
       "louis          0       0     1  ...   0        0    0         0     0   \n",
       "mike           0       0     0  ...   0        0    0         2     0   \n",
       "ricky          1       1     2  ...   0        0    0         0     0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  éclair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 5583 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:33,289 : INFO : using symmetric alpha at 0.5\n",
      "2020-05-02 23:05:33,293 : INFO : using symmetric eta at 0.5\n",
      "2020-05-02 23:05:33,305 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:05:33,317 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:05:33,813 : INFO : -9.178 per-word bound, 579.3 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:33,817 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:05:33,981 : INFO : topic #0 (0.500): 0.003*\"joke\" + 0.003*\"parents\" + 0.003*\"mom\" + 0.003*\"ass\" + 0.002*\"love\" + 0.002*\"friend\" + 0.002*\"door\" + 0.002*\"son\" + 0.002*\"dick\" + 0.002*\"dog\"\n",
      "2020-05-02 23:05:33,985 : INFO : topic #1 (0.500): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"parents\" + 0.002*\"dead\" + 0.002*\"comedy\" + 0.002*\"jokes\" + 0.002*\"door\" + 0.002*\"friend\" + 0.002*\"ahah\" + 0.002*\"wife\"\n",
      "2020-05-02 23:05:33,989 : INFO : topic diff=0.786290, rho=1.000000\n",
      "2020-05-02 23:05:34,573 : INFO : -8.496 per-word bound, 361.1 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:34,573 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:05:34,737 : INFO : topic #0 (0.500): 0.003*\"ass\" + 0.003*\"ok\" + 0.002*\"joke\" + 0.002*\"parents\" + 0.002*\"dog\" + 0.002*\"bo\" + 0.002*\"guns\" + 0.002*\"love\" + 0.002*\"mom\" + 0.002*\"jenny\"\n",
      "2020-05-02 23:05:34,741 : INFO : topic #1 (0.500): 0.005*\"mom\" + 0.004*\"joke\" + 0.003*\"parents\" + 0.003*\"ahah\" + 0.002*\"wife\" + 0.002*\"door\" + 0.002*\"mad\" + 0.002*\"jokes\" + 0.002*\"friend\" + 0.002*\"hasan\"\n",
      "2020-05-02 23:05:34,745 : INFO : topic diff=0.280025, rho=0.577350\n",
      "2020-05-02 23:05:35,241 : INFO : -8.378 per-word bound, 332.6 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:35,245 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:05:35,372 : INFO : topic #0 (0.500): 0.003*\"ass\" + 0.003*\"ok\" + 0.003*\"bo\" + 0.003*\"dog\" + 0.002*\"guns\" + 0.002*\"jenny\" + 0.002*\"um\" + 0.002*\"girls\" + 0.002*\"eye\" + 0.002*\"love\"\n",
      "2020-05-02 23:05:35,377 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.003*\"parents\" + 0.003*\"ahah\" + 0.003*\"wife\" + 0.003*\"door\" + 0.003*\"mad\" + 0.002*\"jokes\" + 0.002*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:35,384 : INFO : topic diff=0.187058, rho=0.500000\n",
      "2020-05-02 23:05:35,848 : INFO : -8.327 per-word bound, 321.2 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:35,852 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:05:35,976 : INFO : topic #0 (0.500): 0.003*\"ass\" + 0.003*\"ok\" + 0.003*\"bo\" + 0.003*\"dog\" + 0.002*\"um\" + 0.002*\"guns\" + 0.002*\"jenny\" + 0.002*\"eye\" + 0.002*\"girls\" + 0.002*\"repeat\"\n",
      "2020-05-02 23:05:35,980 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"wife\" + 0.003*\"door\" + 0.003*\"mad\" + 0.003*\"jokes\" + 0.002*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:35,984 : INFO : topic diff=0.113777, rho=0.447214\n",
      "2020-05-02 23:05:36,476 : INFO : -8.308 per-word bound, 317.0 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:36,480 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:05:36,596 : INFO : topic #0 (0.500): 0.003*\"ass\" + 0.003*\"ok\" + 0.003*\"bo\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"eye\" + 0.002*\"girls\" + 0.002*\"repeat\"\n",
      "2020-05-02 23:05:36,604 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"wife\" + 0.003*\"mad\" + 0.003*\"jokes\" + 0.002*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:36,608 : INFO : topic diff=0.078529, rho=0.408248\n",
      "2020-05-02 23:05:37,064 : INFO : -8.298 per-word bound, 314.7 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:37,068 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:05:37,188 : INFO : topic #0 (0.500): 0.003*\"ass\" + 0.003*\"ok\" + 0.003*\"bo\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"eye\" + 0.002*\"gun\" + 0.002*\"repeat\"\n",
      "2020-05-02 23:05:37,192 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"wife\" + 0.003*\"mad\" + 0.003*\"jokes\" + 0.002*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:37,196 : INFO : topic diff=0.062102, rho=0.377964\n",
      "2020-05-02 23:05:37,640 : INFO : -8.289 per-word bound, 312.8 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:37,644 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:05:37,756 : INFO : topic #0 (0.500): 0.003*\"ass\" + 0.003*\"ok\" + 0.003*\"bo\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.003*\"gun\" + 0.002*\"eye\" + 0.002*\"repeat\"\n",
      "2020-05-02 23:05:37,760 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"wife\" + 0.003*\"mad\" + 0.003*\"jokes\" + 0.002*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:37,764 : INFO : topic diff=0.051630, rho=0.353553\n",
      "2020-05-02 23:05:38,272 : INFO : -8.281 per-word bound, 311.1 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:38,276 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:05:38,368 : INFO : topic #0 (0.500): 0.003*\"ok\" + 0.003*\"ass\" + 0.003*\"bo\" + 0.003*\"dog\" + 0.003*\"gun\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"repeat\" + 0.002*\"eye\"\n",
      "2020-05-02 23:05:38,376 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"mad\" + 0.003*\"wife\" + 0.003*\"jokes\" + 0.002*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:38,380 : INFO : topic diff=0.041220, rho=0.333333\n",
      "2020-05-02 23:05:38,804 : INFO : -8.275 per-word bound, 309.8 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:38,804 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:05:38,916 : INFO : topic #0 (0.500): 0.003*\"ok\" + 0.003*\"ass\" + 0.003*\"bo\" + 0.003*\"gun\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"repeat\" + 0.002*\"eye\"\n",
      "2020-05-02 23:05:38,920 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"jokes\" + 0.003*\"mad\" + 0.003*\"wife\" + 0.003*\"hasan\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:38,924 : INFO : topic diff=0.031880, rho=0.316228\n",
      "2020-05-02 23:05:39,339 : INFO : -8.271 per-word bound, 309.0 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:39,344 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:05:39,463 : INFO : topic #0 (0.500): 0.003*\"ok\" + 0.003*\"ass\" + 0.003*\"bo\" + 0.003*\"gun\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"repeat\" + 0.002*\"eye\"\n",
      "2020-05-02 23:05:39,467 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"jokes\" + 0.003*\"mad\" + 0.003*\"wife\" + 0.003*\"hasan\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:39,467 : INFO : topic diff=0.024356, rho=0.301511\n",
      "2020-05-02 23:05:39,487 : INFO : topic #0 (0.500): 0.003*\"ok\" + 0.003*\"ass\" + 0.003*\"bo\" + 0.003*\"gun\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"repeat\" + 0.002*\"eye\"\n",
      "2020-05-02 23:05:39,491 : INFO : topic #1 (0.500): 0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"jokes\" + 0.003*\"mad\" + 0.003*\"wife\" + 0.003*\"hasan\" + 0.003*\"friend\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"ok\" + 0.003*\"ass\" + 0.003*\"bo\" + 0.003*\"gun\" + 0.003*\"dog\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.002*\"repeat\" + 0.002*\"eye\"'),\n",
       " (1,\n",
       "  '0.005*\"joke\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.003*\"ahah\" + 0.003*\"door\" + 0.003*\"jokes\" + 0.003*\"mad\" + 0.003*\"wife\" + 0.003*\"hasan\" + 0.003*\"friend\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:39,527 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-05-02 23:05:39,531 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-05-02 23:05:39,543 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:05:39,555 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:05:40,371 : INFO : -9.457 per-word bound, 702.9 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:40,375 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:05:40,599 : INFO : topic #0 (0.333): 0.005*\"joke\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"ass\" + 0.003*\"friend\" + 0.002*\"dog\" + 0.002*\"ok\" + 0.002*\"jokes\" + 0.002*\"hell\" + 0.002*\"door\"\n",
      "2020-05-02 23:05:40,603 : INFO : topic #1 (0.333): 0.003*\"mom\" + 0.003*\"parents\" + 0.002*\"door\" + 0.002*\"comedy\" + 0.002*\"hasan\" + 0.002*\"dick\" + 0.002*\"wife\" + 0.002*\"friend\" + 0.002*\"ahah\" + 0.002*\"joke\"\n",
      "2020-05-02 23:05:40,607 : INFO : topic #2 (0.333): 0.003*\"joke\" + 0.002*\"son\" + 0.002*\"comedy\" + 0.002*\"repeat\" + 0.002*\"jokes\" + 0.002*\"mom\" + 0.002*\"door\" + 0.002*\"dead\" + 0.002*\"bo\" + 0.002*\"dick\"\n",
      "2020-05-02 23:05:40,615 : INFO : topic diff=0.988044, rho=1.000000\n",
      "2020-05-02 23:05:41,251 : INFO : -8.637 per-word bound, 398.1 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:41,255 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:05:41,431 : INFO : topic #0 (0.333): 0.005*\"joke\" + 0.004*\"mom\" + 0.003*\"ass\" + 0.003*\"parents\" + 0.003*\"gun\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.002*\"ok\" + 0.002*\"wife\" + 0.002*\"guns\"\n",
      "2020-05-02 23:05:41,435 : INFO : topic #1 (0.333): 0.005*\"ahah\" + 0.004*\"hasan\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"door\" + 0.003*\"son\" + 0.003*\"gay\" + 0.003*\"nigga\" + 0.002*\"comedy\" + 0.002*\"brown\"\n",
      "2020-05-02 23:05:41,439 : INFO : topic #2 (0.333): 0.004*\"bo\" + 0.003*\"joke\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"jokes\" + 0.003*\"contact\" + 0.002*\"brain\" + 0.002*\"comedy\" + 0.002*\"dick\" + 0.002*\"cos\"\n",
      "2020-05-02 23:05:41,447 : INFO : topic diff=0.537251, rho=0.577350\n",
      "2020-05-02 23:05:42,075 : INFO : -8.367 per-word bound, 330.2 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:42,079 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:05:42,191 : INFO : topic #0 (0.333): 0.004*\"joke\" + 0.004*\"mom\" + 0.003*\"ass\" + 0.003*\"parents\" + 0.003*\"gun\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.002*\"wife\" + 0.002*\"guns\"\n",
      "2020-05-02 23:05:42,195 : INFO : topic #1 (0.333): 0.006*\"ahah\" + 0.005*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"door\" + 0.003*\"nigga\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"brown\" + 0.003*\"comedy\"\n",
      "2020-05-02 23:05:42,199 : INFO : topic #2 (0.333): 0.004*\"joke\" + 0.004*\"bo\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"jokes\" + 0.003*\"contact\" + 0.003*\"brain\" + 0.002*\"cos\" + 0.002*\"nuts\" + 0.002*\"jenner\"\n",
      "2020-05-02 23:05:42,207 : INFO : topic diff=0.309093, rho=0.500000\n",
      "2020-05-02 23:05:42,711 : INFO : -8.290 per-word bound, 313.0 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:42,715 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:05:42,815 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"parents\" + 0.003*\"gun\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:42,819 : INFO : topic #1 (0.333): 0.006*\"ahah\" + 0.005*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"door\" + 0.003*\"nigga\" + 0.003*\"gay\" + 0.003*\"brown\" + 0.003*\"son\" + 0.003*\"comedy\"\n",
      "2020-05-02 23:05:42,819 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.003*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"cos\" + 0.003*\"nuts\"\n",
      "2020-05-02 23:05:42,823 : INFO : topic diff=0.176901, rho=0.447214\n",
      "2020-05-02 23:05:43,330 : INFO : -8.265 per-word bound, 307.6 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:43,334 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:05:43,494 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"parents\" + 0.003*\"gun\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:43,498 : INFO : topic #1 (0.333): 0.006*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.003*\"door\" + 0.003*\"gay\" + 0.003*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:43,502 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.003*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"cos\" + 0.003*\"nuts\"\n",
      "2020-05-02 23:05:43,506 : INFO : topic diff=0.105802, rho=0.408248\n",
      "2020-05-02 23:05:43,890 : INFO : -8.256 per-word bound, 305.7 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:43,890 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:05:43,962 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:43,966 : INFO : topic #1 (0.333): 0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.003*\"door\" + 0.003*\"gay\" + 0.003*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:43,970 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"cos\" + 0.003*\"nuts\"\n",
      "2020-05-02 23:05:43,974 : INFO : topic diff=0.065116, rho=0.377964\n",
      "2020-05-02 23:05:44,362 : INFO : -8.252 per-word bound, 304.8 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:44,366 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:05:44,446 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:44,450 : INFO : topic #1 (0.333): 0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.004*\"door\" + 0.003*\"gay\" + 0.003*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:44,454 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"cos\" + 0.003*\"nuts\"\n",
      "2020-05-02 23:05:44,462 : INFO : topic diff=0.041071, rho=0.353553\n",
      "2020-05-02 23:05:44,866 : INFO : -8.250 per-word bound, 304.5 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:44,870 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:05:44,962 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:44,970 : INFO : topic #1 (0.333): 0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.004*\"door\" + 0.004*\"gay\" + 0.003*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:44,974 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"cos\" + 0.003*\"nuts\"\n",
      "2020-05-02 23:05:44,978 : INFO : topic diff=0.026489, rho=0.333333\n",
      "2020-05-02 23:05:45,342 : INFO : -8.250 per-word bound, 304.3 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:45,342 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:05:45,454 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:45,462 : INFO : topic #1 (0.333): 0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.004*\"door\" + 0.004*\"gay\" + 0.004*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:45,466 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"cos\" + 0.003*\"nuts\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:45,470 : INFO : topic diff=0.017437, rho=0.316228\n",
      "2020-05-02 23:05:46,026 : INFO : -8.249 per-word bound, 304.3 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:46,030 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:05:46,154 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:46,162 : INFO : topic #1 (0.333): 0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.004*\"door\" + 0.004*\"gay\" + 0.004*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:46,166 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"nuts\" + 0.003*\"cos\"\n",
      "2020-05-02 23:05:46,170 : INFO : topic diff=0.011697, rho=0.301511\n",
      "2020-05-02 23:05:46,190 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"\n",
      "2020-05-02 23:05:46,194 : INFO : topic #1 (0.333): 0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.004*\"door\" + 0.004*\"gay\" + 0.004*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"\n",
      "2020-05-02 23:05:46,198 : INFO : topic #2 (0.333): 0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"nuts\" + 0.003*\"cos\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"parents\" + 0.003*\"friend\" + 0.003*\"dog\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"guns\"'),\n",
       " (1,\n",
       "  '0.007*\"ahah\" + 0.006*\"hasan\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.004*\"nigga\" + 0.004*\"door\" + 0.004*\"gay\" + 0.004*\"brown\" + 0.003*\"son\" + 0.003*\"bike\"'),\n",
       " (2,\n",
       "  '0.005*\"joke\" + 0.005*\"bo\" + 0.004*\"repeat\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.003*\"contact\" + 0.003*\"jenner\" + 0.003*\"brain\" + 0.003*\"nuts\" + 0.003*\"cos\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's start with 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:46,250 : INFO : using symmetric alpha at 0.25\n",
      "2020-05-02 23:05:46,258 : INFO : using symmetric eta at 0.25\n",
      "2020-05-02 23:05:46,274 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:05:46,290 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:05:46,714 : INFO : -9.796 per-word bound, 889.1 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:46,714 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:05:46,826 : INFO : topic #0 (0.250): 0.006*\"mom\" + 0.004*\"parents\" + 0.003*\"joke\" + 0.002*\"comedy\" + 0.002*\"hasan\" + 0.002*\"door\" + 0.002*\"york\" + 0.002*\"ass\" + 0.002*\"hell\" + 0.002*\"jokes\"\n",
      "2020-05-02 23:05:46,830 : INFO : topic #1 (0.250): 0.003*\"door\" + 0.003*\"parents\" + 0.003*\"joke\" + 0.003*\"mom\" + 0.002*\"friend\" + 0.002*\"ass\" + 0.002*\"son\" + 0.002*\"jenny\" + 0.002*\"love\" + 0.002*\"dog\"\n",
      "2020-05-02 23:05:46,834 : INFO : topic #2 (0.250): 0.003*\"mom\" + 0.003*\"joke\" + 0.003*\"dick\" + 0.003*\"ahah\" + 0.002*\"friend\" + 0.002*\"son\" + 0.002*\"parents\" + 0.002*\"gun\" + 0.002*\"ass\" + 0.002*\"wife\"\n",
      "2020-05-02 23:05:46,846 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.003*\"ass\" + 0.002*\"mom\" + 0.002*\"jokes\" + 0.002*\"um\" + 0.002*\"bo\" + 0.002*\"dog\" + 0.002*\"parents\" + 0.002*\"comedy\" + 0.002*\"young\"\n",
      "2020-05-02 23:05:46,846 : INFO : topic diff=1.197490, rho=1.000000\n",
      "2020-05-02 23:05:47,214 : INFO : -8.879 per-word bound, 470.9 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:47,214 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:05:47,309 : INFO : topic #0 (0.250): 0.008*\"mom\" + 0.006*\"parents\" + 0.004*\"hasan\" + 0.004*\"clinton\" + 0.003*\"york\" + 0.003*\"cow\" + 0.003*\"wife\" + 0.003*\"door\" + 0.003*\"brown\" + 0.003*\"president\"\n",
      "2020-05-02 23:05:47,313 : INFO : topic #1 (0.250): 0.003*\"gun\" + 0.003*\"jenny\" + 0.003*\"ass\" + 0.003*\"guns\" + 0.003*\"class\" + 0.003*\"son\" + 0.003*\"joke\" + 0.003*\"door\" + 0.002*\"morning\" + 0.002*\"hell\"\n",
      "2020-05-02 23:05:47,313 : INFO : topic #2 (0.250): 0.006*\"ahah\" + 0.003*\"gay\" + 0.003*\"nigga\" + 0.003*\"son\" + 0.003*\"friend\" + 0.002*\"wife\" + 0.002*\"mad\" + 0.002*\"young\" + 0.002*\"dick\" + 0.002*\"oj\"\n",
      "2020-05-02 23:05:47,317 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.004*\"bo\" + 0.004*\"jokes\" + 0.003*\"ok\" + 0.003*\"eye\" + 0.003*\"repeat\" + 0.003*\"mom\" + 0.003*\"anthony\" + 0.003*\"dead\" + 0.003*\"um\"\n",
      "2020-05-02 23:05:47,321 : INFO : topic diff=0.676172, rho=0.577350\n",
      "2020-05-02 23:05:47,713 : INFO : -8.473 per-word bound, 355.3 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:47,713 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:05:47,805 : INFO : topic #0 (0.250): 0.008*\"mom\" + 0.006*\"parents\" + 0.005*\"hasan\" + 0.005*\"clinton\" + 0.003*\"york\" + 0.003*\"cow\" + 0.003*\"wife\" + 0.003*\"brown\" + 0.003*\"president\" + 0.003*\"door\"\n",
      "2020-05-02 23:05:47,813 : INFO : topic #1 (0.250): 0.004*\"gun\" + 0.004*\"jenny\" + 0.003*\"guns\" + 0.003*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"son\" + 0.003*\"girlfriend\" + 0.003*\"morning\" + 0.003*\"joke\"\n",
      "2020-05-02 23:05:47,817 : INFO : topic #2 (0.250): 0.009*\"ahah\" + 0.005*\"gay\" + 0.005*\"nigga\" + 0.003*\"son\" + 0.003*\"oj\" + 0.003*\"friend\" + 0.003*\"ghetto\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"mad\"\n",
      "2020-05-02 23:05:47,821 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.004*\"ok\" + 0.004*\"jokes\" + 0.004*\"bo\" + 0.003*\"eye\" + 0.003*\"mom\" + 0.003*\"repeat\" + 0.003*\"anthony\" + 0.003*\"dead\" + 0.003*\"um\"\n",
      "2020-05-02 23:05:47,821 : INFO : topic diff=0.424311, rho=0.500000\n",
      "2020-05-02 23:05:48,185 : INFO : -8.341 per-word bound, 324.2 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:48,189 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:05:48,265 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.003*\"wife\" + 0.003*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:48,269 : INFO : topic #1 (0.250): 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\" + 0.003*\"argument\"\n",
      "2020-05-02 23:05:48,269 : INFO : topic #2 (0.250): 0.010*\"ahah\" + 0.005*\"nigga\" + 0.005*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.003*\"ghetto\" + 0.003*\"young\" + 0.003*\"friend\" + 0.003*\"motherfucker\" + 0.003*\"kevin\"\n",
      "2020-05-02 23:05:48,277 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.004*\"ok\" + 0.004*\"jokes\" + 0.004*\"bo\" + 0.003*\"eye\" + 0.003*\"mom\" + 0.003*\"repeat\" + 0.003*\"anthony\" + 0.003*\"dead\" + 0.003*\"contact\"\n",
      "2020-05-02 23:05:48,281 : INFO : topic diff=0.263983, rho=0.447214\n",
      "2020-05-02 23:05:48,629 : INFO : -8.292 per-word bound, 313.4 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:48,633 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:05:48,701 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:48,701 : INFO : topic #1 (0.250): 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"joke\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:48,705 : INFO : topic #2 (0.250): 0.011*\"ahah\" + 0.006*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:48,709 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.004*\"ok\" + 0.004*\"jokes\" + 0.004*\"bo\" + 0.004*\"eye\" + 0.003*\"mom\" + 0.003*\"repeat\" + 0.003*\"anthony\" + 0.003*\"dead\" + 0.003*\"contact\"\n",
      "2020-05-02 23:05:48,713 : INFO : topic diff=0.173265, rho=0.408248\n",
      "2020-05-02 23:05:49,045 : INFO : -8.268 per-word bound, 308.2 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:49,049 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:05:49,121 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:49,125 : INFO : topic #1 (0.250): 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"joke\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:49,129 : INFO : topic #2 (0.250): 0.011*\"ahah\" + 0.006*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:49,133 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"ok\" + 0.004*\"jokes\" + 0.004*\"bo\" + 0.004*\"eye\" + 0.004*\"mom\" + 0.003*\"repeat\" + 0.003*\"anthony\" + 0.003*\"dead\" + 0.003*\"contact\"\n",
      "2020-05-02 23:05:49,137 : INFO : topic diff=0.117110, rho=0.377964\n",
      "2020-05-02 23:05:49,437 : INFO : -8.254 per-word bound, 305.2 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:49,437 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:05:49,505 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:49,509 : INFO : topic #1 (0.250): 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"joke\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:49,517 : INFO : topic #2 (0.250): 0.012*\"ahah\" + 0.006*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:49,521 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.005*\"ok\" + 0.004*\"bo\" + 0.004*\"jokes\" + 0.004*\"eye\" + 0.004*\"mom\" + 0.003*\"repeat\" + 0.003*\"anthony\" + 0.003*\"contact\" + 0.003*\"dead\"\n",
      "2020-05-02 23:05:49,521 : INFO : topic diff=0.080919, rho=0.353553\n",
      "2020-05-02 23:05:49,801 : INFO : -8.245 per-word bound, 303.5 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:49,805 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:05:49,869 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:49,877 : INFO : topic #1 (0.250): 0.004*\"gun\" + 0.004*\"joke\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:49,881 : INFO : topic #2 (0.250): 0.012*\"ahah\" + 0.006*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:49,881 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.005*\"ok\" + 0.004*\"bo\" + 0.004*\"jokes\" + 0.004*\"mom\" + 0.004*\"eye\" + 0.003*\"repeat\" + 0.003*\"anthony\" + 0.003*\"contact\" + 0.003*\"husband\"\n",
      "2020-05-02 23:05:49,885 : INFO : topic diff=0.056858, rho=0.333333\n",
      "2020-05-02 23:05:50,217 : INFO : -8.240 per-word bound, 302.4 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:50,217 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:05:50,285 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:50,289 : INFO : topic #1 (0.250): 0.004*\"joke\" + 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:50,297 : INFO : topic #2 (0.250): 0.012*\"ahah\" + 0.007*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.004*\"young\" + 0.003*\"motherfucker\" + 0.003*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:50,297 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.005*\"ok\" + 0.004*\"bo\" + 0.004*\"jokes\" + 0.004*\"mom\" + 0.004*\"eye\" + 0.004*\"repeat\" + 0.003*\"anthony\" + 0.003*\"contact\" + 0.003*\"husband\"\n",
      "2020-05-02 23:05:50,301 : INFO : topic diff=0.040428, rho=0.316228\n",
      "2020-05-02 23:05:50,557 : INFO : -8.237 per-word bound, 301.7 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:50,561 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:05:50,613 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:50,617 : INFO : topic #1 (0.250): 0.004*\"joke\" + 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:50,621 : INFO : topic #2 (0.250): 0.012*\"ahah\" + 0.007*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.004*\"young\" + 0.004*\"motherfucker\" + 0.004*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:50,625 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.005*\"ok\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.004*\"repeat\" + 0.003*\"anthony\" + 0.003*\"contact\" + 0.003*\"husband\"\n",
      "2020-05-02 23:05:50,629 : INFO : topic diff=0.028962, rho=0.301511\n",
      "2020-05-02 23:05:50,641 : INFO : topic #0 (0.250): 0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"\n",
      "2020-05-02 23:05:50,645 : INFO : topic #1 (0.250): 0.004*\"joke\" + 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"\n",
      "2020-05-02 23:05:50,649 : INFO : topic #2 (0.250): 0.012*\"ahah\" + 0.007*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.004*\"young\" + 0.004*\"motherfucker\" + 0.004*\"kevin\" + 0.003*\"friend\"\n",
      "2020-05-02 23:05:50,653 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.005*\"ok\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.004*\"repeat\" + 0.003*\"anthony\" + 0.003*\"contact\" + 0.003*\"husband\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"mom\" + 0.007*\"parents\" + 0.006*\"hasan\" + 0.006*\"clinton\" + 0.004*\"cow\" + 0.004*\"york\" + 0.004*\"wife\" + 0.004*\"brown\" + 0.003*\"president\" + 0.003*\"birthday\"'),\n",
       " (1,\n",
       "  '0.004*\"joke\" + 0.004*\"gun\" + 0.004*\"jenny\" + 0.004*\"guns\" + 0.004*\"class\" + 0.003*\"ass\" + 0.003*\"hell\" + 0.003*\"girlfriend\" + 0.003*\"son\" + 0.003*\"morning\"'),\n",
       " (2,\n",
       "  '0.012*\"ahah\" + 0.007*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.004*\"young\" + 0.004*\"motherfucker\" + 0.004*\"kevin\" + 0.003*\"friend\"'),\n",
       " (3,\n",
       "  '0.006*\"joke\" + 0.005*\"ok\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"eye\" + 0.004*\"jokes\" + 0.004*\"repeat\" + 0.003*\"anthony\" + 0.003*\"contact\" + 0.003*\"husband\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's start with 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Topics in Each Document¶\n",
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:50,685 : INFO : using symmetric alpha at 0.25\n",
      "2020-05-02 23:05:50,689 : INFO : using symmetric eta at 0.25\n",
      "2020-05-02 23:05:50,693 : INFO : using serial LDA version on this node\n",
      "2020-05-02 23:05:50,701 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-05-02 23:05:51,053 : INFO : -9.795 per-word bound, 888.1 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:51,057 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-05-02 23:05:51,145 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"parents\" + 0.003*\"mom\" + 0.003*\"dog\" + 0.002*\"friend\" + 0.002*\"dick\" + 0.002*\"jenny\" + 0.002*\"wife\" + 0.002*\"clinton\" + 0.002*\"class\"\n",
      "2020-05-02 23:05:51,145 : INFO : topic #1 (0.250): 0.005*\"joke\" + 0.003*\"mom\" + 0.002*\"parents\" + 0.002*\"comedy\" + 0.002*\"bo\" + 0.002*\"jokes\" + 0.002*\"ass\" + 0.002*\"dead\" + 0.002*\"door\" + 0.002*\"rape\"\n",
      "2020-05-02 23:05:51,149 : INFO : topic #2 (0.250): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"parents\" + 0.002*\"jokes\" + 0.002*\"ass\" + 0.002*\"son\" + 0.002*\"door\" + 0.002*\"mad\" + 0.002*\"wife\" + 0.002*\"friend\"\n",
      "2020-05-02 23:05:51,153 : INFO : topic #3 (0.250): 0.003*\"joke\" + 0.003*\"door\" + 0.003*\"comedy\" + 0.003*\"mom\" + 0.003*\"parents\" + 0.003*\"gun\" + 0.003*\"friend\" + 0.002*\"ahah\" + 0.002*\"son\" + 0.002*\"dick\"\n",
      "2020-05-02 23:05:51,157 : INFO : topic diff=1.211449, rho=1.000000\n",
      "2020-05-02 23:05:51,448 : INFO : -8.843 per-word bound, 459.1 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:51,452 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-05-02 23:05:51,552 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\" + 0.002*\"morning\"\n",
      "2020-05-02 23:05:51,552 : INFO : topic #1 (0.250): 0.006*\"joke\" + 0.003*\"rape\" + 0.003*\"nuts\" + 0.003*\"jenner\" + 0.003*\"jokes\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.002*\"dead\" + 0.002*\"twitter\" + 0.002*\"doctor\"\n",
      "2020-05-02 23:05:51,556 : INFO : topic #2 (0.250): 0.004*\"mom\" + 0.004*\"joke\" + 0.003*\"ok\" + 0.003*\"husband\" + 0.002*\"ass\" + 0.002*\"parents\" + 0.002*\"jokes\" + 0.002*\"anthony\" + 0.002*\"mad\" + 0.002*\"pregnant\"\n",
      "2020-05-02 23:05:51,560 : INFO : topic #3 (0.250): 0.004*\"ahah\" + 0.004*\"mom\" + 0.004*\"joke\" + 0.004*\"hasan\" + 0.003*\"parents\" + 0.003*\"door\" + 0.003*\"son\" + 0.003*\"comedy\" + 0.003*\"gun\" + 0.003*\"mad\"\n",
      "2020-05-02 23:05:51,564 : INFO : topic diff=0.660777, rho=0.577350\n",
      "2020-05-02 23:05:51,876 : INFO : -8.471 per-word bound, 354.8 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:51,880 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-05-02 23:05:51,948 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"bo\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:51,948 : INFO : topic #1 (0.250): 0.008*\"joke\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"rape\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"twitter\" + 0.003*\"jokes\" + 0.003*\"bruce\" + 0.003*\"doctor\"\n",
      "2020-05-02 23:05:51,952 : INFO : topic #2 (0.250): 0.005*\"husband\" + 0.005*\"ok\" + 0.004*\"mom\" + 0.003*\"pregnant\" + 0.003*\"asian\" + 0.003*\"ass\" + 0.002*\"doo\" + 0.002*\"joke\" + 0.002*\"wan\" + 0.002*\"fingers\"\n",
      "2020-05-02 23:05:51,956 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.004*\"ahah\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.003*\"mad\" + 0.003*\"parents\" + 0.003*\"son\" + 0.003*\"door\" + 0.003*\"gun\" + 0.003*\"comedy\"\n",
      "2020-05-02 23:05:51,960 : INFO : topic diff=0.404501, rho=0.500000\n",
      "2020-05-02 23:05:52,236 : INFO : -8.342 per-word bound, 324.4 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:52,236 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-05-02 23:05:52,304 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"bo\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:52,308 : INFO : topic #1 (0.250): 0.008*\"joke\" + 0.005*\"nuts\" + 0.004*\"jenner\" + 0.004*\"hampstead\" + 0.004*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.003*\"bruce\" + 0.003*\"jokes\" + 0.003*\"tweet\"\n",
      "2020-05-02 23:05:52,312 : INFO : topic #2 (0.250): 0.006*\"husband\" + 0.006*\"ok\" + 0.004*\"pregnant\" + 0.004*\"mom\" + 0.003*\"asian\" + 0.003*\"doo\" + 0.003*\"fingers\" + 0.003*\"wan\" + 0.003*\"ass\" + 0.003*\"toilet\"\n",
      "2020-05-02 23:05:52,316 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.003*\"son\" + 0.003*\"gun\" + 0.003*\"door\" + 0.003*\"comedy\"\n",
      "2020-05-02 23:05:52,316 : INFO : topic diff=0.250279, rho=0.447214\n",
      "2020-05-02 23:05:52,580 : INFO : -8.294 per-word bound, 313.8 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:52,584 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-05-02 23:05:52,640 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"bo\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:52,644 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.004*\"hampstead\" + 0.004*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.003*\"bruce\" + 0.003*\"tweet\" + 0.003*\"jane\"\n",
      "2020-05-02 23:05:52,648 : INFO : topic #2 (0.250): 0.007*\"husband\" + 0.006*\"ok\" + 0.005*\"pregnant\" + 0.004*\"mom\" + 0.004*\"asian\" + 0.003*\"doo\" + 0.003*\"fingers\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:52,652 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"door\" + 0.003*\"anthony\"\n",
      "2020-05-02 23:05:52,656 : INFO : topic diff=0.160416, rho=0.408248\n",
      "2020-05-02 23:05:52,932 : INFO : -8.273 per-word bound, 309.4 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:52,936 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-05-02 23:05:52,992 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:52,996 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.004*\"hampstead\" + 0.004*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"\n",
      "2020-05-02 23:05:53,000 : INFO : topic #2 (0.250): 0.007*\"husband\" + 0.006*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"mom\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:53,004 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"\n",
      "2020-05-02 23:05:53,008 : INFO : topic diff=0.104515, rho=0.377964\n",
      "2020-05-02 23:05:53,284 : INFO : -8.264 per-word bound, 307.4 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:53,284 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-05-02 23:05:53,348 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:53,352 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"hampstead\" + 0.005*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"\n",
      "2020-05-02 23:05:53,356 : INFO : topic #2 (0.250): 0.007*\"husband\" + 0.007*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.004*\"mom\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:53,360 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"\n",
      "2020-05-02 23:05:53,360 : INFO : topic diff=0.069016, rho=0.353553\n",
      "2020-05-02 23:05:53,648 : INFO : -8.260 per-word bound, 306.5 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 23:05:53,648 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-05-02 23:05:53,704 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:53,708 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"hampstead\" + 0.005*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"\n",
      "2020-05-02 23:05:53,712 : INFO : topic #2 (0.250): 0.007*\"husband\" + 0.007*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.004*\"mom\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:53,716 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"\n",
      "2020-05-02 23:05:53,720 : INFO : topic diff=0.046171, rho=0.333333\n",
      "2020-05-02 23:05:54,008 : INFO : -8.257 per-word bound, 306.0 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:54,008 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-05-02 23:05:54,068 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:54,072 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"hampstead\" + 0.005*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"\n",
      "2020-05-02 23:05:54,076 : INFO : topic #2 (0.250): 0.008*\"husband\" + 0.007*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.004*\"mom\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:54,088 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"\n",
      "2020-05-02 23:05:54,092 : INFO : topic diff=0.031287, rho=0.316228\n",
      "2020-05-02 23:05:54,328 : INFO : -8.256 per-word bound, 305.8 perplexity estimate based on a held-out corpus of 12 documents with 17686 words\n",
      "2020-05-02 23:05:54,328 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-05-02 23:05:54,372 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:54,376 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"hampstead\" + 0.005*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"\n",
      "2020-05-02 23:05:54,380 : INFO : topic #2 (0.250): 0.008*\"husband\" + 0.007*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.004*\"mom\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:54,380 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"\n",
      "2020-05-02 23:05:54,384 : INFO : topic diff=0.021470, rho=0.301511\n",
      "2020-05-02 23:05:54,396 : INFO : topic #0 (0.250): 0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"\n",
      "2020-05-02 23:05:54,400 : INFO : topic #1 (0.250): 0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"hampstead\" + 0.005*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"\n",
      "2020-05-02 23:05:54,404 : INFO : topic #2 (0.250): 0.008*\"husband\" + 0.007*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.004*\"mom\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"\n",
      "2020-05-02 23:05:54,404 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"ass\" + 0.003*\"dog\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"friend\" + 0.003*\"dick\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"girls\"'),\n",
       " (1,\n",
       "  '0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"hampstead\" + 0.005*\"chimp\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"bruce\" + 0.004*\"tweet\" + 0.004*\"jane\"'),\n",
       " (2,\n",
       "  '0.008*\"husband\" + 0.007*\"ok\" + 0.005*\"pregnant\" + 0.004*\"asian\" + 0.004*\"doo\" + 0.004*\"fingers\" + 0.004*\"mom\" + 0.003*\"wan\" + 0.003*\"toilet\" + 0.003*\"ass\"'),\n",
       " (3,\n",
       "  '0.005*\"joke\" + 0.005*\"ahah\" + 0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"mad\" + 0.004*\"parents\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"door\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's start with 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The words that make sense through the above model are\n",
    "Topic 0: mom, parents\n",
    "\n",
    "Topic 1: husband, wife\n",
    "\n",
    "Topic 2: guns\n",
    "\n",
    "Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'ali'),\n",
       " (3, 'anthony'),\n",
       " (3, 'bill'),\n",
       " (0, 'bo'),\n",
       " (3, 'dave'),\n",
       " (3, 'hasan'),\n",
       " (0, 'jim'),\n",
       " (0, 'joe'),\n",
       " (0, 'john'),\n",
       " (0, 'louis'),\n",
       " (0, 'mike'),\n",
       " (1, 'ricky')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "list(zip([a for [(a,b)] in ldana[corpusna]],data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding\n",
    "\n",
    "Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "\n",
    "Topic 1: husband, wife [Ali, John, Mike]\n",
    "\n",
    "Topic 2: guns [Bill, Bo, Jim]\n",
    "\n",
    "Topic 3: profanity [Dave, Joe]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
